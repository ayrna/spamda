%  LaTeX support: latex@mdpi.com 
%  In case you need support, please attach all files that are necessary for compiling as well as the log file, and specify the details of your LaTeX setup (which operating system and LaTeX version / tools you are using).

%=================================================================
\documentclass[energies,article,submit,moreauthors,pdftex]{Definitions/mdpi} 

% If you would like to post an early version of this manuscript as a preprint, you may use preprint as the journal and change 'submit' to 'accept'. The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, aerospace, agriculture, agriengineering, agronomy, algorithms, animals, antibiotics, antibodies, antioxidants, applsci, arts, asc, asi, atmosphere, atoms, axioms, batteries, bdcc, behavsci , beverages, bioengineering, biology, biomedicines, biomimetics, biomolecules, biosensors, brainsci , buildings, cancers, carbon , catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, children, cleantechnol, climate, clockssleep, cmd, coatings, colloids, computation, computers, condensedmatter, cosmetics, cryptography, crystals, dairy, data, dentistry, designs , diagnostics, diseases, diversity, drones, econometrics, economies, education, electrochem, electronics, energies, entropy, environments, epigenomes, est, fermentation, fibers, fire, fishes, fluids, foods, forecasting, forests, fractalfract, futureinternet, futurephys, galaxies, games, gastrointestdisord, gels, genealogy, genes, geohazards, geosciences, geriatrics, hazardousmatters, healthcare, heritage, highthroughput, horticulturae, humanities, hydrology, ijerph, ijfs, ijgi, ijms, ijns, ijtpp, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jcdd, jcm, jcp, jcs, jdb, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmse, jnt, jof, joitmc, jpm, jrfm, jsan, land, languages, laws, life, literature, logistics, lubricants, machines, magnetochemistry, make, marinedrugs, materials, mathematics, mca, medicina, medicines, medsci, membranes, metabolites, metals, microarrays, micromachines, microorganisms, minerals, modelling, molbank, molecules, mps, mti, nanomaterials, ncrna, neuroglia, nitrogen, notspecified, nutrients, ohbm, particles, pathogens, pharmaceuticals, pharmaceutics, pharmacy, philosophies, photonics, physics, plants, plasma, polymers, polysaccharides, preprints , proceedings, processes, proteomes, psych, publications, quantumrep, quaternary, qubs, reactions, recycling, religions, remotesensing, reports, resources, risks, robotics, safety, sci, scipharm, sensors, separations, sexes, signals, sinusitis, smartcities, sna, societies, socsci, soilsystems, sports, standards, stats, surfaces, surgeries, sustainability, symmetry, systems, technologies, test, toxics, toxins, tropicalmed, universe, urbansci, vaccines, vehicles, vetsci, vibration, viruses, vision, water, wem, wevj

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, benchmark, book, bookreview, briefreport, casereport, changes, comment, commentary, communication, conceptpaper, conferenceproceedings, correction, conferencereport, expressionofconcern, extendedabstract, meetingreport, creative, datadescriptor, discussion, editorial, essay, erratum, hypothesis, interestingimages, letter, meetingreport, newbookreceived, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, supfile, technicalnote, viewpoint
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{xx}
\issuenum{1}
\articlenumber{5}
\pubyear{2019}
\copyrightyear{2019}
%\externaleditor{Academic Editor: name}
\history{Received: date; Accepted: date; Published: date}
%\updates{yes} % If there is an update available, un-comment this line

%% MDPI internal command: uncomment if new journal that already uses continuous page numbers 
%\continuouspages{yes}

%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, calc, indentfirst, fancyhdr, graphicx, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, amsthm, hyphenat, natbib, hyperref, footmisc, geometry, caption, url, mdframed, tabto, soul, multirow, microtype, tikz

\usepackage{textcomp, gensymb}
\usepackage{color}
\usepackage[table]{xcolor}
\usepackage{epstopdf}
\usepackage{subfig}
\usepackage[T1]{fontenc}

\epstopdfsetup{outdir=./figures/}
\definecolor{gray090}{gray}{0.90}

%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{SPAMDA: Software for Pre-processing and Analysis of Meteorological DAta to build datasets}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0001-8849-6036} % Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0001-9773-6783} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorC}{0000-0002-2657-776X} % Add \orcidC{} behind the author's name
\newcommand{\orcidauthorD}{0000-0003-4564-1816} % Add \orcidD{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Antonio Manuel G\'omez-Orellana $^{1,\ddagger}$*, Juan Carlos Fern\'andez $^{1,\ddagger}$*\orcidA{}, Manuel Dorado-Moreno $^{1,\ddagger}$*\orcidB{}, Pedro Antonio Guti\'errez $^{1,\ddagger}$*\orcidC{} and C\'esar Herv\'as-Mart\'inez $^{1,\ddagger}$*\orcidD{}}


% Authors, for metadata in PDF
\AuthorNames{Antonio Manuel G\'omez-Orellana, Juan Carlos Fern\'andez, Manuel Dorado-Moreno, Pedro Antonio Guti\'errez and C\'esar Herv\'as-Mart\'inez}



% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Department of Computer Science and Numerical Analysis, University of Cordoba, 14071, C\'ordoba, Spain.}

% Contact information of the corresponding author
\corres{Correspondence: am.gomez@uco.es (A.M.G.-O); jfcaballero@uco.es (J.C.F.); manuel.dorado@uco.es (M.D.-M.); pagutierrez@uco.es (P.A.G.); chervas@uco.es (C.H.-M.) }

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation 3} 
\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{Meteorological data play an important role for the comprehension of the environment, and they are extensively used to perform environmental learning. Machine Learning (ML) techniques have become a valuable support in many research areas, improving the performance of traditional statistical procedures. However, such techniques require datasets containing information related to the topic under study, which are not always available in an appropriate format. Preparing these datasets implies a lot of time and effort by the researchers. This paper presents a software tool for creating datasets using meteorological observations from two well-known sources of information: the \textit{National Data Buoy Center} (NDBC) and the \textit{National Centers for Environmental Prediction} (NCEP)/\textit{National Center for Atmospheric Research} (NCAR) \textit{Reanalysis Project}. The datasets created by the software are ready to be used as inputs for ML techniques in prediction tasks (classification or regression). As the designed software is able to simplify the creation of the datasets and reduce the time needed for this task, it prevents researchers from performing repetitive technical work, allowing them to concentrate on the study of the meteorological aspects of the data. Therefore, researchers can benefit from it in order to achieve a more efficient exploitation and protection of the environment.}

% Keywords
\keyword{Meteorological data; Reanalysis data; Pre-processing; Creating datasets; Prediction tasks; Marine energy}

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences:
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data:
%\dataset{DOI number or link to the deposited data set in cases where the data set is published or set to be published separately. If the data set is submitted and will be published as a supplement to this paper in the journal Data, this field will be filled by the editors of the journal. In this case, please make sure to submit the data set as a supplement when entering your manuscript into our manuscript editorial system.}

%\datasetlicense{license under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%\setcounter{secnumdepth}{4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{section}{-1} %% Remove this when starting to work on the template.
	\section{Introduction}
		
		A better understanding of the environment is of vital importance for science, contributing not only to more efficient exploitation of natural resources but also to the development of new strategies aimed at its protection. In that sense, meteorological observations provide an essential and valuable source of information which is widely used by researchers to address environmental learning, comprehension, prediction and conservation in numerous oceanic and atmospheric studies of a wide variety of areas (e.g. climate change, agriculture, energy, etc.). Some specific examples of the diversity of fields in which meteorological data can be used in are, among others: directional analysis of sea storms \cite{LAFACE201545}, wind power ramp events prediction \cite{DORADOMORENO2017428}, offshore wind energy assessment \cite{DVORAK20101244}, study of the responses exhibited by plankton to fluid motions \cite{FUCHS2016109}, trends in solar radiation \cite{SILVA20101852} or simulation of extreme near shore sea conditions \cite{GOULDBY201415}. All these studies require a prior data collection and its adaptation to a specific format that allows the interpretation of them.
		
		On the other hand, special purpose software is usually developed to help researchers to advance in their studies related to energy and environmental modelling, becoming a great support for decision-making in the exploitation and protection of the environment. In \cite{CAMARGONOGUEIRA2016361}, a software tool for designing solar water heating systems is developed, which simulates different situations and finds the best technical and economical solution. In \cite{LO2015293}, an integrated simulation tool for the optimum design of bifacial solar panel with reflectors is presented. This tool can also be used to analyse the performance of the solar cells. A framework for data integration of offshore wind farms is implemented in \cite{NGUYEN2013150} in order to facilitate data exchange and improve operation and maintenance practices. In \cite{GUTIERREZ201369}, a tool based on \textit{Fatigue, Aerodynamics, Structures, and Turbulence} (FAST) code for performing design optimisation of offshore wind turbines is presented, which can analyse a massive group of cases by means of its parametric design capability. A design tool for architects based on \textit{Lighting and Thermal} (LT) method is used in \cite{BAKER2013156}, with the purpose of comparing and optimising design solutions in terms of energy and comfort performance. Raabe et al. \cite{RAABE2010213} developed two software tools, \textit{Model of Equilibrium of Bay Beaches} (MEPBAY) and \textit{Coastal Modeling System} (SMC), to support different operational levels of headland-bay beach in coastal engineering projects, and an interactive web application, called the \textit{Stream Hydrology And Rainfall Knowledge System} (SHARKS), for analysis of hydrologic and meteorological time series data that can be used for providing valuable insights to urban hydrologic processes is presented in \cite{BRENDEL2019}.
		
		Marine energy prediction is currently a hot topic where meteorological data is used in. Marine Renewable Energy (MRE) is one of the most important renewable and sustainable energy sources available in our environment \cite{en12050787}, and it includes ocean thermal energy, marine tidal current energy and wave energy, among others. Its benefits and great potential \cite{ZEYRINGER20181281} make it one of the most relevant natural resources, playing a crucial role not only in the reduction of the emission of greenhouse gases but also in all other aspects involved in the difficult challenge of the transition to a low carbon footprint society \cite{BHATTACHARYA2017157, en12091657}. Wave energy exhibits a more stable power supply than wind energy and even solar energy. In recent years \textit{Wave Energy Converters} (WECs) \cite{FALCAO2010899} have been developed to transform this wave energy into electricity \cite{en11092289}. WECs are mechanical devices that convert kinetic energy into electrical energy by means of either the vertical oscillation of waves or the linear motion of them. Nevertheless, waves are difficult to be characterised due to their stochastic nature, because of the influence of a large number of environmental factors that exert on them \cite{ochi1998}. As a consequence of this complexity, many aspects of WEC design, deployment and operation \cite{CROWLEY2018159, Abdelkhalik2016, 6898109} need a proper prediction of waves, in order to maximise the wave energy extraction \cite{en80910370}. For this purpose WECs use wave \textit{flux of energy} ($F_e$) which can be calculated from the two most important wave parameters in this regard: \textit{significant wave height} ($H_s$) and \textit{wave energy period} ($T_e$). Additionally, wave predictions \cite{en11010011} are also helpful for designing offshore structures \cite{CHATZIIOANNOU2017126}, operational works in the sea \cite{DALGIC2015211}, providing technical information to the coastal and coral reef planners in developing countries \cite{CALLAGHAN2018123}, etc.

		Currently, and as a support to traditional study procedures, Machine Learning (ML) techniques \cite{Alpaydin:2004:IML:1036287,Bishop:2006:PRM:1162264} are being widely used in numerous research fields related to classification, regression and optimisation tasks \cite{srivastava2017large}, obtaining significant improvements in the performance of the results. ML methodologies can be used not only by experienced data and computer scientists but also by other researchers. For example, the well-known \textit{Waikato Environment for Knowledge Analysis} (WEKA) \cite{WEKA} software tool provides researchers with a wide collection of ML algorithms. In this way, ML techniques have been already applied to tackle wave characterisation, accurately estimating $H_{s}$ and $T_{e}$ parameters \cite{DURANROSAL2017268, KUMAR2017605}, given that robustness of ML methods can tackle the previously explained difficulties in wave energy prediction. The problem is that, in order to apply these methods, it is essential to obtain datasets with relevant information about the issue under study, used to infer knowledge. Usually, these datasets are not publicly available in a friendly format, and their generation is the first step needed.

		The information to create these datasets can be obtained from meteorological observations, but such information may be available in a inappropriate format and even contain missing values or measurements. Consequently, it is usually required to perform pre-processing tasks for improving the quality of the data, such as the replacement of missing values, outlier detection or data normalisation, among others. Furthermore, if more than one source of information is used to achieve a better characterisation of the problem under study \cite{FERNANDEZ201544, Adams2010}, then a merging or matching process have to be carried out by the researchers to manually create the datasets with the needed information. Moreover, depending on the subject and the ML technique to be applied, or even if the researcher considers other factors in order to improve the results or have more in-depth conclusions, the datasets would have to be updated afterwards. In summary, many important details and different intermediate steps have to be considered when creating suitable datasets for ML techniques, resulting in an extremely tedious task.

		The main purpose of this paper is to present a software tool able to prevent researchers from performing this repetitive work and greatly simplify all the steps involved in the creation of datasets. The meteorological data considered for the tool come from two well-known sources of information: the \textit{National Oceanic and Atmospheric Administration} (NOAA) \textit{National Data Buoy Center} (NDBC) \cite{NOAA} and the \textit{National Centers for Environmental Prediction} (NCEP)/\textit{National Center for Atmospheric Research} (NCAR) \textit{Reanalysis Project} (NNRP or R1) \cite{Kalnay1996, Kistler2001}. The software tool presented in this work is named SPAMDA (Software for Pre-processing and Analysis of Meteorological DAta to build datasets). As SPAMDA performs all this data processing, it reduces the time involving these tasks and allows the researchers focus on the study of the meteorological aspects of the observations. The datasets obtained are ready to be used as input for ML techniques in prediction tasks (classification or regression), although the researchers can use them for other purposes. These datasets contain one or more meteorological variables as inputs and one variable as target (variable to be predicted). The format of the generated datasets will be \textit{Attribute-Relation File Format} (ARFF) \cite{WEKA_ARFF}, which is the one used by WEKA. Besides, the datasets can also be generated in \textit{Comma-Separated Values} (CSV) format, enabling the researchers to use others tools.
		
		Up to now, and to the best of our knowledge, this is the first software tool addressing the problem previously discussed, combining meteorological data from NDBC and NNRP. The advantages that SPAMDA offers to the researchers will be detailed in Section \ref{sec:SPAMDA}, although some of them are briefly summarised below:
		\begin{itemize}[leftmargin=*,labelsep=5.8mm]
			\item The generation of datasets becomes a very easy and customizable task, by means of the selection of different input parameters.
			\item It makes the researcher focus on oceanic and atmospheric studies, without having to worry about mechanical tasks.
			\item It provides information about the quality and quantity of the data.
			\item It avoids possible researcher errors in the intermediate steps of the process of creation of the datasets.
			\item It includes different pre-processing tasks, such as normalisation and missing data recovery.
			\item It facilitates data management and well-organised storage of the datasets.
			\item Its modular design allows the implementation of new functional modules for managing meteorological data from others sources for renewable energy research.
			\item It includes an user-friendly GUI, facilitating and greatly simplifying data management, and it is integrated with the Explorer environment of WEKA.
			\item It is multi-platform, and it can be used on any computer with Java regardless of the operating system.
		\end{itemize}
			
		This paper is organised as follows: Section \ref{sec:DataSources} describes the sources of information used by SPAMDA for creating datasets. Section \ref{sec:SPAMDA} describes in detail the features of the software tool. Section \ref{sec:CaseStudy} shows a case study describing the use of SPAMDA in a practical approach. Section \ref{sec:Conclusions} provides the final conclusions and future work.
		
	\section{Meteorological data sources}\label{sec:DataSources}
		
		The data provided by the above-mentioned sources of information of SPAMDA is described below:
		
		\begin{itemize}[leftmargin=*,labelsep=5.8mm]
			
			\item NDBC is a part of the \textit{National Weather Service} (NWS). NDBC designs, develops, operates, and maintains a network of data collecting buoys (stations). The mission of the network is to collect real-time marine meteorological and oceanographic observations, such as $H_s$, dominant wave period, or wind speed and direction, among others.

			The buoys maintained by NDBC are deployed in the coastal and offshore waters around oceans and seas, and they are equipped with assorted sensors which allow them to perform different measurements. The information collected by the buoys is available on the NDBC website \cite{NOAA_1}, and it is divided into different groups. One of them corresponds to standard meteorological information of the historical data collected by each buoy, which can be downloaded as annual text files and whose format was adopted by NDBC since January 2007 \cite{NOAA_2}. These files contain hourly measurements per day from $00$:$50$ to $23$:$50$ UTC (Universal Time Coordinated) and from $23$:$50$ 31th December of the previous desired year to $22$:$50$ 31th December of the desired year. In Table \ref{tab:measurementsDescription}, a comprehensive measurement description and the corresponding units are provided as a summary for the reader. A fragment of one of these files, which contains the measurements collected during year $2017$ by the buoy identified as \textit{Station 46001} in NDBC, is shown in Figure \ref{fig:fragmentAnnualTexFile}. Each column corresponds to a meteorological variable or attribute, and each row or instance corresponds to the values of the measurements collected by the buoy for each attribute at a specific date and time.

			\begin{figure}[H]
				\centering
				\includegraphics[scale=0.50]{figures/FigureFragmentAnnualTextFile.png}
				\caption{A fragment of an annual text file of the \textit{Station 46001}.}
				\label{fig:fragmentAnnualTexFile}
			\end{figure}
			
			\begin{table}[!ht]
			
				\caption{Measurements descriptions and units of each meteorological variable or attribute collected by the buoys.}
				\label{tab:measurementsDescription}
				\footnotesize
				\centering
				
				\begin{tabular}{ccm{8.60cm}@{\setlength{\tabcolsep}{0pt}}m{0.0cm}}
				
					\cline{1-4}
					
					\textbf{Attribute}&\textbf{Units}&\textbf{Description}&\\[0.30cm]
 
					\cline{1-4}
					
					WDIR & degT & Wind direction (the direction the wind is coming from in degrees clockwise from true North) during the same period used for WSPD. \\
					
					\cellcolor{gray090}WSPD & \cellcolor{gray090} m/s & \cellcolor{gray090} Wind speed (m/s) averaged over an eight-minute period for buoys and a two-minute period for land stations. Reported Hourly. \\
					
					GST &  m/s & Peak 5 or 8 second gust speed (m/s) measured during the eight-minute or two-minute period. \\
					
					\cellcolor{gray090} WVHT & \cellcolor{gray090} m & \cellcolor{gray090} Significant wave height (meters) is calculated as the average of the highest one-third of all of the wave heights during the 20-minute sampling period. \\
					
					DPD & sec & Dominant wave period (seconds) is the period with the maximum wave energy. \\
					
					\cellcolor{gray090}APD & \cellcolor{gray090} sec & \cellcolor{gray090} Average wave period (seconds) of all waves during the 20-minute period. \\
					
					MWD & degT & The direction from which the waves at the dominant period (DPD) are coming. The units are degrees from true North, increasing clockwise, with North as 0 (zero) degrees and East as 90 degrees. \\
					
					\cellcolor{gray090}PRES & \cellcolor{gray090} hPa & \cellcolor{gray090} Sea level pressure (hPa). For C-MAN sites and Great Lakes buoys, the recorded pressure is reduced to sea level using the method described in NWS Technical Procedures Bulletin 291 (11/14/80). \\
					
					ATMP & degC & Air temperature (Celsius degrees). &\\[0.10cm]
					
					\cellcolor{gray090}WTMP & \cellcolor{gray090} degC & \cellcolor{gray090} Sea surface temperature (Celsius degrees). For buoys the depth is referenced to the hull's waterline. For fixed platforms it varies with tide, but is referenced to, or near Mean Lower Low Water (MLLW).\\
					
					DEWP & degC & Dewpoint temperature taken at the same height as the air temperature measurement. \\
					
					\cellcolor{gray090}VIS & \cellcolor{gray090} nmi & \cellcolor{gray090} Station visibility (nautical miles). Note that buoy stations are limited to reports from 0 to 1.6 nmi. \\
					
					TIDE & ft & The water level in feet above or below MLLW. \\
					
					\cline{1-4}
						
				\end{tabular}
			 
			\end{table}
			
			Note that the data collected by the network of buoys may be incomplete due to diverse circumstances such as the weather conditions in which the buoys have to operate, failures or malfunctioning elements of the buoys, among others. Accordingly, it may be the situation that some of the measurements are completely missing (missing date or instance) or partially missing (some measurements not recorded), by a buoy or by a set of buoys, once in a while or over a period of time. It may be also possible that the measurements have been recorded at a time different from the expected one. This aspect have to be taken into account when creating the datasets. This casuistry is explained in detail in Appendix \ref{app:AppendixA}.
			
			\item NNRP provides three-dimensional global reanalysis of numerous meteorological variables (e.g. air temperature, components South-North and West-East of wind speed, relative humidity, pressure, etc.), which is available monthly, daily and every $6$ hours at $00$ Z (Zulu time), $06$ Z, $12$ Z and $18$ Z from $1948$ on a global $2.5\degree$ x $2.5\degree$ grid. Weather observations are from different sources, such as ships, satellites and radar, among others. Reanalysis data is created assimilating such observations using the same climate model throughout the entire reanalysis period in order to reduce the effects of modelling changes on climate statistics. Such information has become a substantial support of the needs of the research community, even more in locations where instrumental (real time) data is not available.
			
			The reanalysis data is available in the NNRP website \cite{NNRP}, which it is accessible through different sections. Such data can be fully (a global $2.5\degree$ x $2.5\degree$ grid) or partially (only the desired reanalysis nodes or sub-grid) downloaded as \textit{Network Common Data Form} (NetCDF) files \cite{NetCDF}, a special binary format for representing scientific data, which provides a description of the file contents and also includes the spatial and temporal properties of the data. Each reanalysis file contains the values of a meteorological variable estimated by a mathematical model for each reanalysis node. For a better understanding, in Figure \ref{fig:subGrid} an approximate representation of a sub-grid containing six reanalysis nodes around the geographical location of a buoy (obtained from NDBC) is shown.
			
			
			\begin{figure}[H]
				\centering
				\includegraphics[scale=0.52]{figures/FigureSubGrid.jpg}
				\caption{Example of a six sub-grid reanalysis nodes around the \textit{Station 46001}.}
				\label{fig:subGrid}
			\end{figure}
			
		\end{itemize}
		
		
		Therefore, with both sources of information, which complement each other, and carrying out a matching process, SPAMDA will create datasets for prediction tasks. In this way, the dataset input variables will be one or more reanalysis variables from NNRP and one or more measurements from NDBC. The dataset output variable will always be one measurement from NDBC.
		
	\section{SPAMDA}\label{sec:SPAMDA}
		
		SPAMDA combines  meteorological information from NDBC and NNRP to obtain new datasets for oceanic and atmospheric studies. In order to do so, SPAMDA manages three different types of datasets, which will be described in detail in the following sections, but are briefly introduced bellow for giving the reader a better general understanding:
			\begin{itemize}[leftmargin=*,labelsep=5.8mm]
				\item \textit{Intermediate datasets}: They contain the meteorological observations from NDBC.
				\item \textit{Pre-processed datasets}: They are obtained as a result of pre-processing tasks performed on the intermediate datasets.
				\item \textit{Final datasets}: Created by merging an intermediate or pre-processed dataset (which contain the information from NDBC) with the reanalysis data from NNRP. This procedure is referenced in SPAMDA as matching process and will be carried out according to the study to be performed (classification or regression).
			\end{itemize}
		
		\begin{figure}[H]
			\centering
			\includegraphics[scale=0.32]{figures/FigureSPAMDA.eps}
			\caption{Brief outline of the functionality provided by SPAMDA.}
			\label{fig:SPAMDA}
		\end{figure}
		SPAMDA consists of three main functional modules, whose main features, represented in Figure \ref{fig:SPAMDA}, are the following:
		\begin{itemize}[leftmargin=*,labelsep=5.8mm]
			
			
			\item \textit{Manage buoys data}: The aim of this module is to provide features for the management and analysis of the information related to the buoys from NDBC. This includes:
			\begin{enumerate}[leftmargin=*,labelsep=4.9mm]
				\item Entering and updating the information of each buoy.
				\item Creation of intermediate datasets with the collected measurements.
				\item Pre-processing tasks for obtaining the pre-processed datasets.
				\item Matching process to merge the information from NDBC and NNRP
				\item Creation of the final datasets accordingly to the ML technique to use (classification or regression).
			\end{enumerate}
			
			\item \textit{Manage reanalysis data}: This module is used for the management of the reanalysis data provided by the NNRP. In this way, the researchers can keep the reanalysis data files updated for their studies. Such files will be used, depending on the researchers needs, in the matching process when obtaining the final datasets.
			
			\item \textit{Tools}: This module includes features for converting intermediate or pre-processed datasets to ARFF or CSV format and for opening ARFF files with WEKA software.
			
		\end{itemize}

		In the following subsections each integrated functional module is described in detail.

			\subsection{Buoys}\label{sec:Buoys}
			
				When a new buoy is included in SPAMDA the following information, which can be obtained from NDBC, is requested:
				\begin{itemize}[leftmargin=*,labelsep=5.8mm]
					\item \textit{Station ID}: An alphanumeric identifier that allows easy identification of the buoy.
					\item \textit{Description}: A short description of the buoy.
					\item \textit{Latitude}: North or South geographical localisation (degrees) of the buoy.
					\item \textit{Longitude}: West or East geographical localisation (degrees) of the buoy.
					\item \textit{Measurements files}: The above-mentioned annual text files of the standard meteorological information collected by the buoy and downloaded from the NDBC website. This will be used for the creation of the intermediate datasets. One file per year is expected.
				\end{itemize}
				
				For clarification, an example is presented in Figure \ref{fig:buoys}, where the buoy ID1 has three annual text files and the buoy ID2 has two annual text files.
				
				\begin{figure}[H]
					\centering
					\includegraphics[scale=0.42]{figures/FigureBuoys.eps}
					\caption{Example of entering two buoys with its annual text files.}
					\label{fig:buoys}
				\end{figure}


			\subsection{Datasets}\label{sec:Datasets}
			
				Once a buoy has been included as described in Section \ref{sec:Buoys}, it is possible to create datasets with one or more annual text files, which are referenced in SPAMDA as intermediate datasets. In this module, the researchers can manage intermediate datasets of each buoy, which are the baseline for their studies, by creating new ones or deleting the unnecessary ones.
				
				When an intermediate dataset is created, it is associated with its corresponding buoy. Besides, a summary of its content is also created, providing relevant information such as the number of instances, the dates of the first and last measurements, the annual text files included, and the missing and duplicated dates.
				
				An example where three intermediate datasets have been created is presented in Figure \ref{fig:datasets}. The two intermediate datasets of the buoy ID1 contain meteorological data of different years, and the intermediate dataset of the buoy ID2 contains meteorological data of two years. For each buoy, as many intermediate datasets as needed can be created.
				
				\begin{figure}[H]
					\centering
					\includegraphics[scale=0.40]{figures/FigureDatasets.eps}
					\caption{Example of the creation of the intermediate datasets.}
					\label{fig:datasets}
				\end{figure}
				
				
			\subsection{Pre-process} \label{sec:Preprocess}
				
				Data pre-processing prepares the raw data (intermediate datasets) to be able to be treated correctly by ML algorithms. In this way, the quality of dat can be improved prior to the learning phase, by applying pre-processing tasks (filters). The result will be referenced as pre-processed datasets.
				
				SPAMDA provides several filters grouped in three categories, \textit{Attribute}, \textit{Instance} and \textit{Recover missing data}, including the configuration of their parameters and a short description of them:
				
				\begin{itemize}[leftmargin=*,labelsep=5.8mm]

				 \item \textit{Attribute}: All theses filters can be applied to the attributes (variables of the buoy from NDBC) of the intermediate dataset.
				 
					\begin{itemize}[leftmargin=*,labelsep=5.8mm]
						\item \textit{Normalize} \cite{WEKA_Filter_Normalize}: This filter normalises all numeric values of each attribute. The resulting values are by default in the interval [0,1].
						\item \textit{Remove} \cite{WEKA_Filter_Remove}: It removes an attribute or a range of them.
						\item \textit{RemoveByName} \cite{WEKA_Filter_RemoveByName}: It removes attributes based on a regular expression matched against their names.
						\item \textit{ReplaceMissingValues} \cite{WEKA_Filter_ReplaceMissingValues}: For each attribute, all the missing values will be replaced by the average value of the attribute.
						\item \textit{ReplaceMissingWithUserConstant} \cite{WEKA_Filter_ReplaceMissingWithUserConstant}: This filter replaces all the missing values of the attributes with an user-supplied constant value.
					\end{itemize}
				 
				 \item \textit{Instance}: All theses filters can be applied to the instances (hourly measurements of the buoy from NDBC) of the intermediate dataset.
					\begin{itemize}[leftmargin=*,labelsep=5.8mm]
						\item \textit{RemoveDuplicates} \cite{WEKA_Filter_RemoveDuplicates}: With this filter, all duplicated instances are removed.
						\item \textit{RemoveWithValues} \cite{WEKA_Filter_RemoveWithValues}: This filter removes all the instances that match the attribute and the value supplied by the user.
						\item \textit{SubsetByExpression} \cite{WEKA_Filter_SubsetByExpression}: It removes all the instances which do not match a user-specified expression.
					\end{itemize}
				 
				 \item \textit{Recover missing data}: All these filters can be applied to the instances of the intermediate dataset.
					\begin{itemize}[leftmargin=*,labelsep=5.8mm]
						\item \textit{Replace missing values with next nearest hour}: The missing values of each attribute are replaced with the next nearest non missing value.
						\item \textit{Replace missing values with previous nearest hour}: This filter replaces the missing values of each attribute with the previous nearest non missing value.
						\item \textit{Replace missing values with next $n$ hours mean}: The missing values of each attribute are replaced with the next $n$ nearest non missing values mean, where $n$ can be configured by the user.
						\item \textit{Replace missing values with previous $n$ hours mean}: This filter replaces the missing values of each attribute in the intermediate dataset with the previous $n$ nearest non missing values mean.
						\item \textit{Replace missing values with symmetric $n$ hours mean}: The missing values of each attribute in the intermediate dataset are replaced with the $n$ previous and $n$ next non missing values mean.
					\end{itemize}
				 
				\end{itemize}
				
				SPAMDA allows the researchers to undo the last filter applied or to restore the initial content of the intermediate dataset. Besides, the content and relevant statistical information (number of instances with missing values, minimum and maximum values, mean and standard deviation) of the intermediate and the pre-processed datasets can be visualised in this module.
				
				Figure \ref{fig:preprocess} shows an example where the intermediate datasets 1 and 2 of the buoy ID1 have been pre-processed, obtaining as a result the pre-processed dataset 1 of each one. The intermediate dataset 1 of the buoy ID2 has been also pre-processed. \textit{Pre-processed dataset n} represents that the researchers can create as many pre-processed datasets as they consider opportune.
				
				\begin{figure}[H]
					\centering
					\includegraphics[scale=0.36]{figures/FigurePreprocess.eps}
					\caption{Example of the creation of pre-processed datasets.}
					\label{fig:preprocess}
				\end{figure}
				
				
			\subsection{Matching configuration}
			\label{sec:matching_conf}
			
				In order to merge and format the data provided by the two sources of information described in Section \ref{sec:DataSources}, it is necessary to carry out a matching process. The matching procedure is performed using an intermediate or pre-processed dataset, which includes the measurements collected by a buoy from NDBC, and the needed reanalysis data files from NNRP. Note that SPAMDA is able to manage the NetCDF binary format for handling the information stored in the reanalysis files.
				
				Such process merges the information of both sources that match on time, but, given that the measurements of the buoys are hourly collected from $00$:$50$ to $23$:$50$ UTC, and the reanalysis data is available every $6$ hours at $00$ Z, $06$ Z, $12$ Z and $18$ Z, the matching can only be carried every $6$ hours (discarding the rest of measurements from the buoy data). Besides, and since there is still a difference of 10 minutes, the matching with the reanalysis data will be performed with the nearest buoy measurement (before or after) within a maximum of 60 minutes of difference. Finally, the matched instances of both sources will form the final datasets.
				
				Figure \ref{fig:matchingProcess} presents an example of matching with the measurements collected during $2017$ by \textit{Station 46001} (NDBC) and the reanalysis data (NNRP) of the variable \textit{pressure} for reanalysis nodes $57.5$ N $\times$ $147.5$ W and $55.0$ N $\times$ $147.5$ W in the same year. In this way, only the instances from both sources that are linked with arrows (highlighted in green colour) will be used in the creation of the final datasets. Although the reanalysis dates have been presented in a human readable format, note that reanalysis dates are stored in hours from $01$-$01$-$1800$, and they have to be transformed for comparison taking into account the time zone. Such transformation is automatically done by SPAMDA when matching the instances.
				
				The reader can check in Appendix \ref{app:AppendixA} an example with a more complex case of the procedure.
				\begin{figure}[H]
					\centering
					\includegraphics[scale=0.38]{figures/FigureMatchingProcess.png}
					\caption{An example of matching the data from NDBC (left) and NNRP (right).}
					\label{fig:matchingProcess}
				\end{figure}
				
				SPAMDA allows the researchers to perform a customisable matching process, for obtaining as many different versions of the same meteorological data as needed.
				Prediction tasks are based on the estimation of the output attribute using the information provided by the input attributes. Depending on the task, the datasets must be prepared and configured differently:				
				\begin{itemize}[leftmargin=*,labelsep=5.8mm]
					\item \textit{Classification}: The final datasets will be ready to use as input for ML classifiers, requiring a nominal output attribute, whose specific preparation is detailed in Section \ref{sec:FinalDatasets}.
					\item \textit{Regression}: The final datasets will be ready to use as input for regression methods, requiring a real output attribute, whose preparation is also explained in Section \ref{sec:FinalDatasets}.
					\item \textit{Direct matching}: In this case the inputs attributes have a direct correspondence with the output attribute, and it is not necessary to perform any additional preparation. Both input and target attributes are synchronised in time, in such a way that the final dataset is not intended for prediction purposes. For example, the final datasets may be used in lost data recovering tasks, in correlation studies, in descriptive analyses, etc.
				\end{itemize}
				
				The following parameters can be specified for the matching process:
				\begin{itemize}[leftmargin=*,labelsep=5.8mm]
				
					\item \textit{Flux of energy} \cite{FERNANDEZ201544}: When the $F_e$ is selected, it will be used as output. This attribute is not collected by the buoys, but it can be calculated from two wave parameters: $H_s$ and $T_e$, which are collected as WVHT and APD attributes, respectively, and were described in Table \ref{tab:measurementsDescription}. In this way, SPAMDA obtains the $F_e$ of each instance using the following equation:
					\begin{equation}
							F_e = 0.49 \cdot H^2_s \cdot T_e,
							\label{eq:fluxOfEnergy}
					\end{equation}
						
					where $F_e$ is measured in kilowatts per meter, $H_s$ is measured in meters and $T_e$ is measured in seconds. Note also that $F_e$ is defined in Equation \ref{eq:fluxOfEnergy} as an average energy flux ($H_s$ is a kind of average wave height), though for simplicity it will be referred just as flux of energy.
					
					\item \textit{Attribute to predict}: Instead of using $F_e$, researchers can select any of the attributes collected by the buoys as output (e.g. significant wave height, WVHT, wind direction, WDIR, sea level pressure, PRES, etc.). Therefore, they can conduct different studies by selecting one attribute or other.

					\item \textit{Reanalysis data files}: In order to have a possible better description of the problem under study, more than one reanalysis variable can be considered as input. Remember that these files have to be previously downloaded from the NNRP website \cite{NNRP}, which should set the range of dates (temporal properties) and the desired sub-grid (spatial properties, see Figure \ref{fig:subGrid}) for each variable of reanalysis.
					
					In that sense, the reanalysis data files must have the same spatial and temporal properties but related to different variables. SPAMDA simplifies this task by showing the reanalysis data files that are compatibles each other, and checking that the selection made by the researches meets that condition.
					
					\item \textit{Buoys attributes}: In addition to the reanalysis variables, the final datasets will also include the selected attributes as inputs (of the intermediate or pre-processed dataset used), providing a possible better characterisation of the problem under study, although it will depend on how correlated the attributes are.

					\item \textit{Include missing dates}: As above-mentioned, the information collected by a buoy may be incomplete due to measurements not recorded by it. As a consequence, the matching of instances between both sources of information may not be possible (missing dates). In that situation, the researchers can consider two options: 1) discard the instances affected or 2) include them. In the latter case, the final datasets will contain the affected instances, but the measurements of the buoy will be stored as missing values in WEKA format, denoted as \guillemotleft\textit{?}\guillemotright.
					
					\item \textit{Nearest reanalysis nodes to consider}: As already shown in Figure \ref{fig:subGrid} (which represents six reanalysis nodes), the reanalysis data files may contain information of several reanalysis nodes. In this way, the researcher can:
					
						\begin{itemize}[leftmargin=*,labelsep=5.8mm]
							
							\item Consider all the reanalysis nodes contained in each file: in this case, the information provided by each reanalysis node contained in each selected reanalysis data file will be used.
							
							\item Consider only some of the reanalysis nodes contained in each file: in this case, only the information of the $N$ closest reanalysis nodes to the buoy will be used  ($N$ given by the user). To do that, SPAMDA uses the \textit{Haversine} equation \cite{Haversine_2009} to calculate the distance from each reanalysis node to the location of the buoy and obtain the closest ones. Haversine equation is also known as the great circle distance and performs calculation from main point to destination point with a trigonometric function using latitude and longitude:
								\begin{eqnarray}
									\label{eq:Haversine}
									d(p_0,p_j) & = & \arccos(\sin(lat_0)\cdot \sin(lat_j) \nonumber \\
									& & \cdot \cos(lon_0-lon_j) + \cos(lat_0) \\
									& & \cdot \cos(lat_j)), \nonumber 
								\end{eqnarray}								
							where $p_0$ is the buoy geographical location, $p_j$ stands for the location of each reanalysis node, and $lat$ and $lon$ are the latitude and longitude of the points, respectively.
						\end{itemize}
					
					\item \textit{Number of final datasets}: Depending on the number of nearest reanalysis nodes to consider, the number of final datasets to create and the content of them can be configured according to the following options:
						\begin{itemize}[leftmargin=*,labelsep=5.8mm]
						
							\item \textit{One (using weighted mean of the $N$ nearest reanalysis nodes)}: Only one final dataset will be created, which will contain the attributes (the selected one as output and the selected ones as inputs) of the intermediate or pre-processed dataset used, along with a weighted mean of each variable of the reanalysis data used (one per selected reanalysis data file). This weighted mean is obtained by SPAMDA and uses Equation \ref{eq:Haversine} to obtain the distance from each reanalysis node to the location of the buoy. Once the distances have been calculated they are inverted and normalised as follows:							
								\begin{linenomath*}
									\begin{equation}
										w_i=\frac{\sum_{j=1}^{N} d(p_0,p_j)}{d(p_0,p_i)}, ~~i=1, \ldots, N.
										\label{eq:weightedMean}
									\end{equation}
								\end{linenomath*}

							With these weights, a weighted mean of each variable of reanalysis is obtained for each of the $N$ nodes. Therefore, the closest reanalysis nodes to the localisation of the buoy will provide more information.
							
							Considering as example the two nearest reanalysis nodes represented in Figure \ref{fig:subGrid} and the reanalysis variables air temperature and pressure, the weighted mean of each reanalysis variable will be calculated using the reanalysis nodes $57.5$ N $\times$ $147.5$ W and $55.0$ N $\times$ $147.5$ W.
							
							\item \textit{'N' (one per each reanalysis node)}: As many final datasets as the number of nearest $N$ reanalysis nodes configured by the researcher will be created. Therefore, each final dataset will contain the value of each reanalysis variable used of the nearest corresponding reanalysis node, along with the selected attributes of the intermediate or pre-processed dataset used.
							
							In this case, and considering as example the four closest reanalysis nodes (see Figure \ref{fig:subGrid}) and the reanalysis variables air temperature and pressure, four final datasets will be created, containing each one the information of both reanalysis variables of the corresponding reanalysis node: $57.5$ N $\times$ $147.5$ W, $55.0$ N $\times$ $147.5$ W, $57.5$ N $\times$ $150.0$ W and $55.0$ N $\times$ $150.0$ W, along with the selected attributes of the intermediate or pre-processed dataset used.
							
						\end{itemize}
					
				\end{itemize}
						
				Once the matching parameters have been described, for a better understanding of them, Figure \ref{fig:directMatching} presents an example of the matched information considering the data shown in Figure \ref{fig:matchingProcess} and using the following matching configuration\footnote{Note that the date is shown just for better understanding, but it will not be included in the final dataset.}:
					\begin{itemize}[leftmargin=*,labelsep=5.8mm]
						\item Variable WVHT as attribute to predict.
						\item Variable Pres as reanalysis input attribute.
						\item Variable WSDP as buoy input attribute.
						\item Not including missing dates.
						\item Considering the closest reanalysis node.
						\item Task to be used: \textit{Direct matching}.
					\end{itemize}
						
				\begin{figure}[H]
					\centering
					\includegraphics[scale=0.49]{figures/FigureDirectMatching.png}
					\caption{An example of the matched information for \textit{Direct matching}.}
					\label{fig:directMatching}
				\end{figure}
				
				
			\subsection{Final datasets} \label{sec:FinalDatasets}

				Once the matching process has been performed with the desired configuration, it is necessary to prepare the matched information for the desired prediction task (\textit{Regression} or \textit{Classification}), obtaining as a result the final datasets. Remember that \textit{Direct matching}, as it was described in Section \ref{sec:matching_conf}, performs a direct correspondence between the attributes used as inputs and the output one, and it is not necessary to carry out any preparation.
				
				SPAMDA allows the researchers to make such preparation by means of the following options:
				
					\begin{itemize}[leftmargin=*,labelsep=5.8mm]

						\item \textit{Prediction horizon} (Classification and Regression): This option indicates the time gap for moving backward the attribute to predict (output attribute). In this way, the input attributes (variables of the buoy and reanalysis data) will be used to predict the output attribute in a specific future time (e.g. +6h, +12h, +18h, +1 day, etc.).
						
						The minimum interval for increasing and decreasing the prediction horizon is $6$h (due to reanalysis data temporal resolution) \cite{DORADOMORENO2017428}, the same interval used when the matching process is carried out. Therefore, for each increment of the prediction horizon, an instance is lost of the dataset is lost (as this future information is not available). As the minimum prediction horizon is $6$h, at least one instance will be lost. The relation between the inputs and the attribute to predict will be defined as follows:
						\begin{linenomath*}
							\begin{equation}
								o_{t+\Delta t}=\phi(\mathbf{b}_t,\mathbf{r}_{t}),
								\label{eq:noSynchronisingRD}
							\end{equation}
						\end{linenomath*}
						where $t$ represents the time instant to study and $\Delta t$ the prediction horizon; $o$ is the attribute to be predicted, $\mathbf{b}_t$ is the vector containing the selected NDBC variables and $\mathbf{r}_t$ is the vector containing the selected reanalysis variables. In this way and considering the matched information shown in Figure \ref{fig:directMatching}, WVHT is $o$, the vector $\mathbf{b}$ contains the variable WSPD and the vector $\mathbf{r}$ contains Pres.
						
						Optionally, the reanalysis variables can be synchronised with the attribute to predict. Given that these variables are estimated by a mathematical model, we can obtain very good future estimations, which can improve the performance of the results. In this case, the relation between the inputs and the attribute to predict would be:						
						\begin{linenomath*}
							\begin{equation}
								o_{t+\Delta t}=\phi(\mathbf{b}_t,\mathbf{r}_{t+\Delta t}).
								\label{eq:synchronisingRD}
							\end{equation}
						\end{linenomath*}
						
						Note that the selected NDBC variables as input cannot be synchronised with the attribute to predict.
						
 						For better understanding, considering the matched information shown in Figure \ref{fig:directMatching}, an example of the creation of one \textit{Regression} dataset is shown in Figure \ref{fig:regressionNoSync}. As mentioned earlier, this prediction task requires a real output variable (in this case, WVHT, the last one). The options considered for the preparation of each final dataset are the following:
 							\begin{itemize}[leftmargin=*,labelsep=5.8mm]
 								\item Do not synchronise the reanalysis data (see Equation \ref{eq:noSynchronisingRD} for the relation between the inputs and the output).
 								\item A prediction horizon of $6$h.
 							\end{itemize}
 							 							
							\begin{figure}[H]
								\centering
								\includegraphics[scale=0.49]{figures/FigureRegressionNoSync.png}
								\caption{An example of the creation of a \textit{Regression} dataset, with a prediction horizon of $6$h and without synchronisation.}
								\label{fig:regressionNoSync}
							\end{figure}
 						
 						Note that, due to prediction horizon is $6$h, the values of WVHT attribute are moved backward one instance (up). As a consequence, the last instance ($2017$/$12$/$31$ $18$:$00$) is lost and is not included in the final dataset. Besides, and because the reanalysis data has not been synchronised, the values of the Pres and WSPD variables are at the same time instant ($t$ in Equation \ref{eq:noSynchronisingRD}).
						
 						Moreover, considering again the matched information shown in Figure \ref{fig:directMatching}, an example of the creation of the same dataset but applying synchronisation (see Equation \ref{eq:synchronisingRD}) is shown in Figure \ref{fig:regressionSync}.
 						
							\begin{figure}[H]
								\centering
								\includegraphics[scale=0.49]{figures/FigureRegressionSync.png}
								\caption{An example of the creation of a \textit{Regression} dataset, with a prediction horizon of $6$h and with synchronisation.}
								\label{fig:regressionSync}
							\end{figure}

						Again, and due to the prediction horizon selected ($6$h), the values of the WVHT attribute are moved backward one instance (up) and the last instance ($2017$/$12$/$31$ $18$:$00$) is not included in the final dataset. But now, the values of the Pres variable are also moved backward one instance (due to the synchronisation). Therefore, in this case, Pres is at the same time instant as the attribute to predict (${t+\Delta t}$ in Equation \ref{eq:synchronisingRD}).
						
						\item \textit{Thresholds of the output attribute} (Classification): Since the values of the variables collected by the buoys are real numbers, it is necessary to discretise them (convert them from real to nominal values) for the attribute selected as output (attribute to be predicted). SPAMDA allows the researchers to perform this process by defining the necessary classes with their thresholds, which will be used to carry out such discretisation.
						
						
						Considering again the matched information shown in Figure \ref{fig:directMatching}, an example of the creation of a \textit{Classification} dataset is shown in Figure \ref{fig:prediction}. The options considered for the preparation of the final dataset are the following:
							\begin{itemize}[leftmargin=*,labelsep=5.8mm]
								\item Do not synchronise the reanalysis data.
								\item A prediction horizon of $6$h.
								\item The thresholds shown in Table \ref{tab:thresholds}.
							\end{itemize}
							
						\begin{table}[!h]
						
							\caption{Thresholds for the classification example represented in Figure \ref{fig:prediction}}
							\label{tab:thresholds}
							\footnotesize
							\centering

							\begin{tabular}{cm{3.20cm}cc@{\setlength{\tabcolsep}{0pt}}m{0.0cm}}
							
								\cline{1-5}
								
								\textbf{Class}&\textbf{Description}&\textbf{Inferior [}&\textbf{Superior )}&\\[0.20cm]
			
								\cline{1-5}
								
								Low & Low wave height & $0.36$ & $1.5$&\\[0.15cm]
								
								\cellcolor{gray090}Average & \cellcolor{gray090}Average wave height & \cellcolor{gray090}$1.5$ & \cellcolor{gray090}$2.5$&\\[0.15cm]
								
								Big & Big wave height & $2.5$ & $4.0$&\\[0.15cm]
								
								\cellcolor{gray090}Huge & \cellcolor{gray090}Huge wave height & \cellcolor{gray090}$4.0$ & \cellcolor{gray090}$9.9$&\\[0.15cm]

								\cline{1-5}
									
							\end{tabular}
						
						\end{table}

							
						\begin{figure}[H]
							\centering
							\includegraphics[scale=0.49]{figures/FigureClassification.png}
							\caption{An example of the creation a \textit{Classification} dataset, with a prediction horizon of $6$h and without synchronisation.}
							\label{fig:prediction}
						\end{figure}
						
						Note that the attribute to be predicted has been renamed to \textit{Class\_WVHT} to show that it is now a nominal variable, because its values have been discretised according to the thresholds. Besides, and due to the $6$h prediction horizon, the last instance is lost ($2017$/$12$/$31$ $18$:$00$) and the values of the attribute \textit{Class\_WVHT} are moved backward one instance (up). As the reanalysis data have not been synchronised, the values of the Pres and WSPD variables are at the same time instant ($t$ in Equation \ref{eq:noSynchronisingRD}).
						
					\end{itemize}
				
				The content of the final datasets, obtained as the result of the preparation of the matched data, can be visualised to check everything before saving them on disk. Such preparation can be performed as many times as required and considering the different options in each moment. Although the date will not be included in the final datasets, it can be shown to properly check the matching.
				
				Finally, it is necessary to define the output configuration to create the final datasets:			
				\begin{itemize}[leftmargin=*,labelsep=5.8mm]

					\item \textit{Output path file}: Name of the final datasets and folder to save them on disk.
						
					\item \textit{Final datasets format}:

						\begin{itemize}[leftmargin=*,labelsep=5.8mm]
						
							\item \textit{ARFF}: \textit{Attribute-Relation File Format} \cite{WEKA_ARFF}, which is used by WEKA. SPAMDA allows the researchers to directly open the final datasets in the Explorer environment of WEKA (in the same context of work), enabling them to choose the most appropriate ML method to tackle the problem under study.

							\item \textit{CSV}: \textit{Comma-Separated Values}. This format is included in order to consider other different tasks of software tools.
							
						\end{itemize}
					
				\end{itemize}
				
				A text file that summarises the configuration used in matching process and in the preparation of the matched data is also generated. It can be saved and loaded, enabling the researchers to resume their studies at any other time.
				
				
			\subsection{Manage reanalysis data}
				
				As mentioned in Section \ref{sec:DataSources}, the reanalysis data files provided by NNRP contain the estimated values by a mathematical model of one meteorological variable.
				
				In this module (see Figure \ref{fig:SPAMDA}), SPAMDA includes features for entering new files and deleting the unnecessary ones. Besides, useful information about the content of each reanalysis file can be consulted such as name of the file and the reanalysis variable, number of instances and reanalysis nodes, initial and final time, latitude and longitude. All these fields summarise the temporal and spatial properties of the data. Thus, the researcher can quickly and easily identify each reanalysis file entered in SPAMDA.
				
				An example where two reanalysis data files have been entered in SPAMDA is shown in Figure \ref{fig:manageReanalisys}.
				
				\begin{figure}[H]
					\centering
					\includegraphics[scale=0.45]{figures/FigureManageReanalisys.eps}
					\caption{Example of entering two reanalysis data files.}
					\label{fig:manageReanalisys}
				\end{figure}
				
			\subsection{Tools}
			
				SPAMDA also contains another module that provides two utilities: one of them is \textit{Dataset converter} used for converting the desired intermediate or pre-processed datasets to ARFF or CSV formats; the other utility can be used for opening ARFF files with WEKA Explorer environment, which is useful for easily checking the results of different configurations of the pre-processing.
				
	\section{Case study}\label{sec:CaseStudy}
			
		Although the software includes a user manual, this section will describe how the application works in a practical approach. To do so, an example showing how to create a fully processed dataset (final dataset), starting from the raw data, will be described. The objective of this final dataset is to be used with ML algorithms to classify waves in the Gulf of Alaska depending on their height. The data collected to perform this example is:
		\begin{enumerate}[leftmargin=*,labelsep=4.9mm]
		\item The measurements obtained from 2013 to 2017 by the buoy with ID 46001, placed in the Gulf of Alaska, which are provided by NDBC as annual text files. This data is publicly available at the NDBC website. 
		\item Complementary information collected from reanalysis data containing air temperature, pressure and two components of wind speed measurements (South-North and West-East). This information will be collected from the four closest reanalysis nodes surrounding the geographical location of the buoy. This data is publicly available at the NNRP website and can be downloaded in NetCDF format.
		\end{enumerate}
		
		After gathering the information described above\footnote{Further instructions for downloading this data can be found in the user manual of the application.}, the researcher can open SPAMDA.
		
		In Figure \ref{fig:main_view}, the main window is shown. In order to input the reanalysis data which will be used in further steps for creating the final dataset, the researcher has to select the option \textcolor{blue}{\textit{Manage reanalysis data}}.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.60\textwidth]{figures/FigureMain_view.png}
			\caption{SPAMDA main window.}\label{fig:main_view}
		\end{figure}
		
		Then, the window of Figure \ref{fig:reanalysis} is shown. Here, using the buttons located at the bottom, it is possible to add, delete or consult any data from the different reanalysis files. After the information has been introduced in the application, this window can be closed and the user can go back to the main window to continue entering the information related to the buoy under study. 
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.89\textwidth]{figures/FigureManage_reanalysis_data.png}
			\caption{\textit{Manage reanalysis data} window.}\label{fig:reanalysis}
		\end{figure}
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.89\textwidth]{figures/FigureManage_buoys.png}
			\caption{\textit{Buoys} tab.}\label{fig:manage_buoys}
		\end{figure}
		
		The researcher has to select \textcolor{blue}{\textit{Manage buoys data}} to open the window shown in Figure \ref{fig:manage_buoys}. In this tab, the researcher can consult, modify, add or delete different data related to the buoy. In order to enter such data, click on the \textcolor{blue}{\textit{New}} button, and then the window shown in Figure \ref{fig:new_buoy} pops up.
		
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.70\textwidth]{figures/FigureNew_buoy.png}
			\caption{\em{Entering a new buoy} window.}\label{fig:new_buoy}
		\end{figure}
		
		Here the information about the buoy has to be included: the \textit{Station ID}, its description, geographical localisation and the corresponding annual text files. In this case, the files containing the data from year $2013$ to $2017$ are inserted by clicking on the \textcolor{blue}{\textit{Add file}} button. Once the data has been introduced, it is necessary to click on the \textcolor{blue}{\textit{Save}} button to insert the buoy in SPAMDA database. After that, the window can be closed.  
		
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.90\textwidth]{figures/FigureDatasets.png}
			\caption{\textit{Datasets} tab.}\label{fig:show_datasets}
		\end{figure}
		
		To create the intermediate dataset, the researcher has to double-click on the buoy under study or click on the \textcolor{blue}{\textit{Datasets}} tab to switch to the corresponding view (see Figure \ref{fig:show_datasets}). In this view, the researcher can delete or consult a summary of each intermediate or pre-processed dataset by selecting it from the corresponding list. It can also create new ones. To proceed with the creation of the intermediate dataset, the user clicks on the \textcolor{blue}{\textit{New}} button, and the view shown in Figure \ref{fig:intermediate} appears. 
		
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.70\textwidth]{figures/FigureNew_intermediate_dataset.png}
			\caption{\textit{New intermediate dataset} view.}\label{fig:intermediate}
		\end{figure}
		
		Here the researcher can select the annual text files to be included in the intermediate dataset. In this case, all the files introduced before, which correspond to the buoy under study, are selected. When the file selection is finished, \textcolor{blue}{\textit{Create}} button has to be clicked in order to introduce the description and the file name of the current intermediate dataset, and then, with the \textcolor{blue}{\textit{Save}} button, the creation process starts, showing the status of the process during it.
		
		After that, in order to prepare the intermediate dataset, the dataset is selected, and then the button \textcolor{blue}{\textit{Open}} is clicked to jump to the tab \textcolor{blue}{\textit{Pre-process}} (shown in Figure \ref{fig:preprocess_data}). In this tab, relevant statistical information about the selected dataset is shown, and also the content of the dataset can be consulted, providing the researcher the capacity to evaluate the pre-processing being performed.
		
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.90\textwidth]{figures/FigurePreprocess.png}
			\caption{\textit{Pre-process} tab.}\label{fig:preprocess_data}
		\end{figure}
		
		Here the researcher can apply (and configure) the necessary filters (explained in Section \ref{sec:Preprocess}) to the selected dataset, and, in the bottom part, the main statistics of the dataset are displayed, which can be used to observe the changes produced when applying a filter. As mentioned earlier, this case study is focused on classifying waves considering their height, so any missing data from wave height ($376$ values) and the remaining attributes are recovered, using the filter \textit{Replace missing values with symmetric $3$ hours mean}. Furthermore, the attributes MWD, DEWP, VIS and TIDE are removed from the dataset by applying the filter \textit{RemoveByName}, since the first two had more than $92$\% of missing data and the last two $100$\%. After finishing the pre-processing of the dataset, the researcher can click on the \textcolor{blue}{\textit{Save}} button, to introduce the description and file name for the current pre-processed dataset.
		
		At this point, the researcher has registered the buoy in SPAMDA, then entered its raw data and selected the required data for the problem (intermediate dataset). Finally, the data has been pre-processed in order to be ready for its future use in ML algorithms. In order to achieve a more accurate description of the problem under study, a matching process can be carried out to merge the processed data from NDBC with the reanalysis data (also entered previously) from NNRP. The next step is to click on the \textcolor{blue}{\textit{Matching configuration}} tab, to open the view shown in  Figure \ref{fig:matching_conf}. 
		
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.90\textwidth]{figures/FigureMatching_configuration.png}
			\caption{\textit{Matching configuration} tab.}\label{fig:matching_conf}
		\end{figure}
		
		In this view, the researcher can customise (or load) the parameters of the matching process according to their needs and select the prediction task  (described in Section \ref{sec:matching_conf}) that the final dataset will be used for. In this example, the following parameters were selected:
		\begin{itemize}[leftmargin=*,labelsep=5.8mm]
			\item Attribute to predict: WVHT.
			\item Reanalysis data: Air, pressure, u-wind and v-wind.
			\item Buoy attributes to be used as inputs: WDIR, WSPD, GST, DPD, APD, PRES, ATMP and WTMP.
			\item Reanalysis nodes to consider: $1$.
			\item Number of final datasets: In this example that option is disabled, because only one reanalysis node is considered.
			\item Prediction task: Classification.
		\end{itemize} 
		
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.90\textwidth]{figures/FigureFinal_datasets.png}
			\caption{\textit{Final datasets} tab.}\label{fig:final_dataset}
		\end{figure}
		
		After configuring the matching process, the researcher can click on the \textcolor{blue}{\textit{Run}} button to jump to the view shown in Figure \ref{fig:final_dataset} and proceed to define the final dataset structure according to the selected prediction task. Given that, in the previous window, \textit{Classification} was selected, the researcher can now add, modify or delete the thresholds (usually defined by an expert) for discretising the output variable. After this, the next step is to set the time horizon desired and also to activate (if desired) the synchronisation (in time) of reanalysis variables with the output, as explained in Section {\ref{sec:FinalDatasets}}. Then the researcher can click on the \textcolor{blue}{\textit{Update final dataset}} button to see the content shown in the bottom left corner. Finally, after checking that everything is correct, the last step would be to select the name (and path) of the dataset file and its output format and click on the \textcolor{blue}{\textit{Create final datasets}} button. For this case study, the following configuration was applied:		
		\begin{itemize}[leftmargin=*,labelsep=5.8mm]
			\item Thresholds: see Table \ref{tab:thresholds}.
			\item Prediction horizon: 6 hours
			\item Synchronisation: Disabled
		\end{itemize}
		
		At this point the final dataset would be created and stored in the computer of the researcher. Also, there is an option to open the dataset with WEKA (after creating it) in order to perform a first classification approach or a preliminary study of the data structure, as shown in Figure \ref{fig:openigFinalDatasetWeka}.
		
		\begin{figure}[ht!]
			\centering
			\includegraphics[width=0.95\textwidth]{figures/FigureOpeningFinalDatasetWeka.png}
			\caption{Final dataset opened with the environment Explorer of WEKA.}
			\label{fig:openigFinalDatasetWeka}
		\end{figure}
		
		
	\section{Conclusions}\label{sec:Conclusions}

		A software tool for creating datasets using meteorological data from NDBC and NNRP has been presented. These datasets will be ready to use as input for ML techniques in prediction tasks (classification or regression). As a result, the researchers will benefit from a great support when carrying out their oceanic and atmospheric studies, related to energy and environmental modelling. Moreover, given that SPAMDA simplifies all the intermediate steps involved in the creation of datasets (such as entering the meteorological information, managing with the incomplete data, pre-processing tasks, the customisable matching process to merge the data and the preparation of the datasets according to the ML technique to use), it avoids errors and reduces the time needed. In this way, the researchers will be able to have more in-depth analysis, which could result in more complete conclusions about the issue under study.
		
		In order to improve SPAMDA, some future work could be focused on new functional modules for managing meteorological data of different formats \cite{NOAA_3}, so that the developed tool can be extended to any other research, new pre-processing functionalities such as filters to analyse the correlation between attributes or new functional modules for recovering missing values using nearby buoys data \cite{DuranRosal2016}. Furthermore, the developed software could manage other sources of reanalysis data (with different spatial and temporal resolution), and new output formats for the datasets which could be used as input by other tools for ML such as KEEL (\textit{Knowledge Extraction based on Evolutionary Learning}) \cite{AlcalFdez2009KEELAS}. However, such new functionalities can be developed with a reasonable effort to be able to manage each particular casuistry. For example, when dealing with incomplete data, interpreting different data and files structures or carrying out the matching process of two environmental data sources.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\supplementary{The following are available online at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title.}

% Only for the journal Methods and Protocols:
% If you wish to submit a video article, please do so with any other supplementary material.
% \supplementary{The following are available at \linksupplementary{s1}, Figure S1: title, Table S1: title, Video S1: title. A supporting video article is available at doi: link.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{All authors contributed to this work. Design and implementation of the software, A.M.G.-O., J.F.C. and M.D.-M.; Test of the software, P.A.G. and C.H.-M.; Conceptualization, A.M.G.-O., J.F.C., M.D.-M., P.A.G., and C.H.-M.; Methodology, A.M.G.-O., J.F.C., M.D.-M., P.A.G., and C.H.-M.; Investigation, A.M.G.-O., J.F.C., M.D.-M., P.A.G., and C.H.-M.; Writing-original draft preparation, A.M.G.-O., J.F.C. and M.D.-M.; Funding acquisition, P.A.G. and C.H.-M.; Resources, P.A.G. and C.H.-M.; Supervision, P.A.G. and C.H.-M.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\funding{This work has been partially subsidised by the projects TIN2017-85887-C2-1-P and TIN2017-90567-REDT of the Spanish Ministry of Economy and Competitiveness (MI\-NE\-CO), and FEDER funds of the European Union.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\acknowledgments{We thank to NVIDIA Corporation for the transfer of computational resources for research works. The authors also thank to NOAA/OAR/ESRL PSD, Boulder, Colorado, USA for the NCEP Reanalysis data provided from their Web site at \url{https://www.esrl.noaa.gov/psd/}, to NOAA/NDBC by its data that were collected and made freely available, to University of Waikato for the Weka (Waikato Environment for Knowledge Analysis) software tool, to University Corporation for Atmospheric Research/Unidata for the NetCDF (network Common Data Form) Java library and to QOS.ch for the SLF4J (Simple Logging Facade for Java) library.}

 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\conflictsofinterest{The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
\abbreviations{The following abbreviations are used in this manuscript:\\

\noindent 
\begin{tabular}{@{}ll}
$F_e$ & Flux of energy\\
$H_s$ & Significant wave height\\
$T_e$ & Wave energy period\\
$p_0$ & Geographical location of the buoy\\
$p_j$ & Geographical location of each reanalysis node\\
$lat$ & Latitude of the point\\
$lon$ & Longitude of the point\\
$o_{t}$ & The attribute to be predicted at the time instant to study\\
$\Delta t$ & The prediction horizon\\
$\mathbf{b}_t$ & The vector containing the selected NDBC variables\\
$\mathbf{r}_{t}$ & The vector containing the selected reanalysis variables\\
\end{tabular}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
\appendixtitles{yes} %Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendix
\section{Managing incomplete data}\label{app:AppendixA}
%\unskip
%\subsection{}

		In this appendix, we describe how SPAMDA deals with incomplete data when creating intermediate datasets and performing the matching process.
		
		The measurements collected by the buoys may be incomplete or recorded at a different time than the expected one, due to the weather conditions in which the buoys have to operate. To illustrate this casuistry, the following examples are shown in Figure \ref{fig:measurements}:
		\begin{itemize}[leftmargin=*,labelsep=5.8mm]
			\item In the instance marked with a), the measurement of $17$:$50$ was collected at $17$:$45$, $5$ minutes earlier.
			\item In the instance marked with b), the measurement of $23$:$50$ was collected at $23$:$30$, $20$ minutes earlier.
			\item In the instance marked with c), the measurement of $05$:$50$ is duplicated.
			\item In the instance marked with d), the measurement of $11$:$50$ is missing (missing date or instance).
			\item In the instance marked with e), the measurement of $17$:$50$ and $18$:$50$ are missing (missing dates or instances).
			\item Missing values highlighted in red colour.
		\end{itemize}
		
		\begin{figure}[ht!]
			\centering
			\includegraphics[scale=0.45]{figures/FigureMeasurements.png}
			\caption{A fragment of an annual text file with different missing value examples.}
			\label{fig:measurements}
		\end{figure}
		
		SPAMDA has been designed to tackle these situations, and it informs the researchers of any incidence found while reading the annual text files for creating the intermediate datasets. For the case of measurements that were recorded at a different time than expected, it has been established a time gap of 6 minutes ($10\%$ of an hour). Therefore, if the time difference exceeds such value the date will be considered as an unexpected.
		
		Figure \ref{fig:creatingDataset} shows the status of the creation of an intermediate dataset with the information of Figure \ref{fig:measurements}. Note that the instance marked with a) has not been informed by SPAMDA as an unexpected date because its time difference is less than $6$ minutes. Depending on the affected attribute, NDBC uses a specific value \cite{NOAA_3} to indicate the presence of lost data (e.g. $99$ for VIS and TIDE attributes, $999$ for DEWP, MWD and WDIR, etc.). SPAMDA interprets these specific values and, after creating the intermediate dataset, the researchers can check if it contains missing values by visualising its statistical information or content. Remember that SPAMDA provides several filters for recovering missing data, which were described in Section \ref{sec:Datasets}.
		
		\begin{figure}[ht!]
			\centering
			\includegraphics[scale=0.40]{figures/FigureCreatingDataset.png}
			\caption{Status of the creation of the intermediate dataset for the example of Fig \ref{fig:measurements}.}
			\label{fig:creatingDataset}
		\end{figure}
		
		SPAMDA takes into account this casuistry when carrying out the matching process. An example is given in Figure \ref{fig:matchingMeasurements}. As above-mentioned, the matching process is performed with the nearest measurement (previous or next) within a maximum of 60 minutes of difference. However, in the instance marked with $e)$, given that the measurements dates $01$/$05$/$2017$ $17$:$50$ and $01$/$05$/$2017$ $18$:$50$ are missing, the reanalysis date $01$/$05$/$2017$ $18$:$00$ cannot be matched with buoy data (this date is highlighted in mauve colour in the Figure). Depending on the selection made by the researchers in the parameter \textit{Include missing dates}, this instance will be included in the final dataset (with missing values for buoy variables) or not.
		
		\begin{figure}[ht!]
			\centering
			\includegraphics[scale=0.36]{figures/FigureMatchingMeasurements.png}
			\caption{Matching the measurements (left) and the reanalysis data (right).}
			\label{fig:matchingMeasurements}
		\end{figure}


%\section{}
%All appendix sections must be cited in the main text. In the appendixes, Figures, Tables, etc. should be labeled starting with `A', e.g., Figure A1, Figure A2, etc. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: internal bibliography
%=====================================
\reftitle{References}
%\begin{thebibliography}{999}
% Reference 1
%\bibitem[Author1(year)]{ref-journal}
%Author1, T. The title of the cited article. {\em Journal Abbreviation} {\bf 2008}, {\em 10}, 142--149.
% Reference 2
%\bibitem[Author2(year)]{ref-book}
%Author2, L. The title of the cited contribution. In {\em The Book Title}; Editor1, F., Editor2, A., Eds.; Publishing House: City, Country, 2007; pp. 32--58.
%\end{thebibliography}

% The following MDPI journals use author-date citation: Arts, Econometrics, Economies, Genealogy, Humanities, IJFS, JRFM, Laws, Religions, Risks, Social Sciences. For those journals, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%=====================================
% References, variant B: external bibliography
%=====================================
\externalbibliography{yes}
\bibliography{bibfile}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% optional
%\sampleavailability{Samples of the compounds ...... are available from the authors.}

%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors response\\
%Reviewer 2 comments and authors response\\
%Reviewer 3 comments and authors response
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

